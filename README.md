# RAG-problem


Entire project is done in 1 python file ( rag.py )

Problem:
The main objective of this problem is to design a system that can search
and summarize vast amounts of textual data efficiently. The candidate should
be able to utilize a Large Language Model (LLM) like GPT-4 or its successors to
accomplish this task.

Here I am using gpt-4o-mini model for the summarization

The project is a streamlit program as seen below, input API key first to be able to proceed further
<img width="500" height="280" alt="Screenshot (118)" src="https://github.com/user-attachments/assets/ca9f230f-7484-4edd-9581-34ec114f9ee3" />
<img width="500" height="280" alt="Screenshot (119)" src="https://github.com/user-attachments/assets/ae68a7da-d3ca-45eb-b7c2-4834c3c6c671" />


After this you can set the word limit for the summary and then upload the file which the program parses and the llm is ready to give the summary 
<img width="500" height="280" alt="Screenshot (120)" src="https://github.com/user-attachments/assets/29e671c2-2526-4c28-b88d-d65e955688b3" />

Now it gives how many sections present in the document and then you can get the summary. It is recommended to get summary for each section as seen below
<img width="500" height="280" alt="Screenshot (121)" src="https://github.com/user-attachments/assets/3647ecab-0aa6-464a-af29-9858c69151e1" />
<img width="500" height="280" alt="Screenshot (122)" src="https://github.com/user-attachments/assets/a764e183-f88d-478f-900b-c72e42a26823" />


